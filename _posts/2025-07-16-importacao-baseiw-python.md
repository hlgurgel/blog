---
title: "Importa√ß√£o eficiente de arquivos gigantescos no Python com pyodbc, pandas e logging"
date: 2025-07-16 17:00:00 -0300
categories: [python, banco-de-dados, automacao, data-engineering]
---

Neste post, vamos compartilhar a solu√ß√£o que desenvolvemos para lidar com a **importa√ß√£o autom√°tica de arquivos `.xlsx` e `.csv` com milh√µes de linhas** em uma base SQL Server, usando **Python**.

---

## üéØ O problema

Precis√°vamos importar arquivos fornecidos por terceiros com at√© 1 milh√£o de linhas (ou mais), contendo informa√ß√µes estruturadas como datas, identificadores, e nomes de pontos de venda. Os arquivos podiam estar em:

- `.csv`
- `.xls`
- `.xlsx`

Al√©m do volume massivo de dados, encontramos os seguintes desafios:

- **Formatos de datas inconsistentes**
- **Linhas incompletas ou inv√°lidas**
- **Possibilidade de duplicatas**
- **Necessidade de performance**
- **Agendamento recorrente e seguro**

---

## üìÖ Desafio: campo de data com m√∫ltiplos formatos

Os arquivos chegavam com datas em padr√µes mistos, como:

- `06/05/2025` (dd/MM/yyyy)
- `6/28/25` (MM/dd/yy)
- `2025-06-28` (ISO)

Para isso usamos:

```python
df_valid['DATA_ATIVACAO'] = pd.to_datetime(df_valid['DATA_ATIVACAO'], dayfirst=True, errors='coerce')
```

O par√¢metro `dayfirst=True` nos protege contra interpreta√ß√µes erradas em arquivos vindos do Brasil. O `errors='coerce'` transforma erros de convers√£o em `NaT`, permitindo a filtragem das linhas inv√°lidas depois.

---

## üßπ Detec√ß√£o de linhas inv√°lidas

Validamos cada linha verificando se **todas as colunas obrigat√≥rias** est√£o presentes e preenchidas. Se algo estiver vazio, registramos a linha como inv√°lida no log com o motivo.

Isso garante que:
- As linhas que entram no banco est√£o corretas
- Arquivos corrompidos ou mal formatados n√£o travam o processo

---

## ‚öôÔ∏è pyodbc vs pandas.to_sql: por que escolhemos o pyodbc?

Embora o `pandas.to_sql()` seja mais simples, **ele n√£o respeita a cl√°usula `IGNORE_DUP_KEY` do SQL Server** ‚Äî o que significa que ele falha ao tentar inserir duplicatas.

Com o `pyodbc`, conseguimos usar:

```python
cursor.fast_executemany = True
cursor.executemany(insert_sql, insert_data)
```

E o SQL Server ignora as duplicatas silenciosamente, mantendo a integridade da tabela.

---

## üöÄ Uso do `fast_executemany` para alta performance

O `fast_executemany = True` √© uma configura√ß√£o do `pyodbc` que permite enviar grandes volumes de dados de forma vetorizada, evitando m√∫ltiplas roundtrips entre Python e o SQL Server.

Sem ele, inserir 1 milh√£o de linhas pode levar horas.
Com ele, o mesmo processo leva minutos.

---

## üì¶ Divis√£o em lotes: controle e feedback

Mesmo com `executemany`, dividimos a inser√ß√£o em **lotes de 10.000 linhas**, assim:

- O banco de dados n√£o trava com um bloco gigante
- Podemos mostrar no log o progresso para o usu√°rio
- Facilitamos o diagn√≥stico em caso de falha (sabemos em qual lote parou)

Exemplo:

```python
for i in range(total_batches):
    logger.info(f"Inserindo lote {i + 1}/{total_batches}...")
    cursor.executemany(insert_sql, batch)
```

---

## üìì Logging estruturado com `logging`

Usar o m√≥dulo `logging` do Python foi essencial para:

- Criar arquivos `.log` por arquivo processado
- Registrar erros com detalhes
- Acompanhar o progresso com `watch -n 10`

Exemplo:

```python
logging.basicConfig(filename='arquivo.log', level=logging.INFO)
logger.info("Iniciando processamento do arquivo X...")
```

---

## üïí Agendamento com seguran√ßa: `crontab + flock`

Agendamos o script para rodar a cada 5 minutos com `crontab`, mas usamos `flock` para evitar execu√ß√µes simult√¢neas:

```cron
*/5 * * * * flock -n /tmp/baseiw.lock /home/ubuntu/base-iw/venv/bin/python /home/ubuntu/base-iw/import_baseiw.py
```

üîó Veja nosso post completo sobre isso: [Evite concorr√™ncia em scripts agendados com crontab usando flock](/linux/automacao/shell/crontab/2025/07/16/flock-crontab-concorrencia.html)

---

## üîó Refer√™ncias √∫teis

- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html)
- [pyodbc docs](https://github.com/mkleehammer/pyodbc/wiki)
- [fast_executemany explicado](https://github.com/mkleehammer/pyodbc/wiki/fast_executemany)
- [logging ‚Äî Logging facility for Python](https://docs.python.org/3/library/logging.html)
- [Post anterior sobre `flock` e `crontab`](https://github.com/hlgurgel/blog/blob/main/_posts/2025-07-16-flock-crontab-concorrencia.md)

---

## ‚úÖ Conclus√£o

Com essa arquitetura, conseguimos:

- Processar arquivos gigantescos com robustez
- Evitar erros de data e de duplicidade
- Garantir rastreabilidade com logs
- Escalar o processo com seguran√ßa

Ficou com d√∫vidas ou quer adaptar isso para sua realidade? Comente l√° no [reposit√≥rio do blog](https://github.com/hlgurgel/blog) ou me chame por aqui.
